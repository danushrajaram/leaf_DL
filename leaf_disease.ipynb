{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHkThs79ZZ82"
      },
      "outputs": [],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDZC23mtSVna"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install tensorflow keras matplotlib pillow numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ai4Gs_cBSN2q"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "# Import ImageDataGenerator from tensorflow.keras.preprocessing.image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85eS158SSYzK"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install Kaggle API and set up credentials\n",
        "!pip install kaggle --upgrade\n",
        "\n",
        "# Upload your kaggle.json\n",
        "from google.colab import files\n",
        "files.upload()  # Upload kaggle.json here\n",
        "\n",
        "# Set up Kaggle API credentials\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l833WzqSTfd1"
      },
      "outputs": [],
      "source": [
        "# Step 2: Download the dataset from Kaggle\n",
        "!kaggle datasets download -d rashikrahmanpritom/plant-disease-recognition-dataset\n",
        "\n",
        "# Step 3: Extract the downloaded dataset\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "dataset_zip = 'plant-disease-recognition-dataset.zip'\n",
        "dataset_dir = '/content/Dataset'\n",
        "\n",
        "with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_dir)\n",
        "\n",
        "print(f\"Dataset extracted to {dataset_dir}\")\n",
        "\n",
        "# Check dataset structure\n",
        "!ls {dataset_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OjHjg7ZScXZ"
      },
      "outputs": [],
      "source": [
        "# Updated dataset paths\n",
        "train_path = os.path.join(dataset_dir, \"Train/Train\")\n",
        "validation_path = os.path.join(dataset_dir, \"Validation/Validation\")\n",
        "test_path = os.path.join(dataset_dir, \"Test/Test\")\n",
        "\n",
        "# Function to count files\n",
        "def total_files(folder_path):\n",
        "    return len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
        "\n",
        "# Display dataset stats\n",
        "print(\"Training dataset statistics:\")\n",
        "for label in [\"Healthy\", \"Powdery\", \"Rust\"]:\n",
        "    print(f\" - {label}: {total_files(os.path.join(train_path, label))}\")\n",
        "\n",
        "print(\"\\nValidation dataset statistics:\")\n",
        "for label in [\"Healthy\", \"Powdery\", \"Rust\"]:\n",
        "    print(f\" - {label}: {total_files(os.path.join(validation_path, label))}\")\n",
        "\n",
        "print(\"\\nTest dataset statistics:\")\n",
        "for label in [\"Healthy\", \"Powdery\", \"Rust\"]:\n",
        "    print(f\" - {label}: {total_files(os.path.join(test_path, label))}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXB2LUG7Un34"
      },
      "outputs": [],
      "source": [
        "# Data augmentation and preprocessing\n",
        "# Instead of importing from keras.preprocessing.image, import from tensorflow.keras.preprocessing.image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1.0 / 255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_path, target_size=(225, 225), batch_size=32, class_mode='categorical')\n",
        "validation_generator = test_datagen.flow_from_directory(validation_path, target_size=(225, 225), batch_size=32, class_mode='categorical')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vZb0reKUuMo"
      },
      "outputs": [],
      "source": [
        "# Model creation\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(225, 225, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(3, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN_AdLcXU38a"
      },
      "outputs": [],
      "source": [
        "# Model compilation\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(train_generator, epochs=5, validation_data=validation_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj8H7pAHU65I"
      },
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcMqNqpvU_Wo"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save('leaf_disease_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeI_7WTZWNK5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "\n",
        "# Function to preprocess an image\n",
        "def preprocess_image(image_path, target_size=(225, 225)):\n",
        "    img = load_img(image_path, target_size=target_size)\n",
        "    x = img_to_array(img)\n",
        "    x = x.astype('float32') / 255.0\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    return x\n",
        "\n",
        "# Allow user to upload an image\n",
        "print(\"Please upload an image for prediction:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Check if an image is uploaded\n",
        "if uploaded:\n",
        "    # Get the uploaded image file name\n",
        "    uploaded_image_path = list(uploaded.keys())[0]\n",
        "\n",
        "    # Display the uploaded image\n",
        "    uploaded_image = Image.open(uploaded_image_path)\n",
        "    plt.imshow(uploaded_image)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Uploaded Image\")\n",
        "    plt.show()\n",
        "\n",
        "    # Preprocess the uploaded image\n",
        "    x = preprocess_image(uploaded_image_path)\n",
        "\n",
        "    # Predict using the trained model\n",
        "    predictions = model.predict(x)\n",
        "\n",
        "    # Decode the prediction\n",
        "    labels = {v: k for k, v in train_generator.class_indices.items()}\n",
        "    predicted_label = labels[np.argmax(predictions)]\n",
        "    print(f\"Predicted label: {predicted_label}\")\n",
        "\n",
        "    # Display the prediction result\n",
        "    print(f\"The uploaded image is predicted to be: {predicted_label}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
